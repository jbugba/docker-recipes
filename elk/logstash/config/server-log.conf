########################################################################
##############                  INPUT
########################################################################

input {
  file {
    # le asignamos el type para que sea más fácil el armado del evento y del kibana asociado
    type => "server-log"

    # el path tiene que estar separado por directorios por hosts,
    # cuando se instale logstash en cada server esto se cambia y "*" contendrá el valor del server que publicó el evento
    path => "/opt/logstash/logs/*/server.log*"
    exclude => "*.gz"

    # path donde guardamos la bd de lectura de archivos
    sincedb_path => "/opt/logstash/sincedb/sincedb_server-log"

    # cuando ponemos beginning, empieza los files desde cero SIEMPRE Y CUANDO hayamos borrado el registro en $HOME/.sincedb_*
    # Si .sincedb* existe, lleva un registro de donde se estaban leyendo los files y cuales
    # Falta debugear para saber bien como "engañar" solo algunos files
#    start_position => "beginning"

    # Si empieza con una fecha, es un multiline event, sino, hay qye "pegarselo" al anterior
    #codec => multiline {
    #  pattern => "^%{TIMESTAMP_ISO8601} "
  #    negate => true
#   what => previous
#    }
  }
}


########################################################################
##############                  FILTER
########################################################################

filter {
    # Todos los eventos tienen un ^timestamp, loglevel y class
    grok {
      match => [ "message", "^%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{LOGLEVEL:level}%{SPACE}\[%{JAVACLASS:class}\]" ]
    }

    # Este es el timestamp estandar de jboss
    grok {
      match => [ "message", "^%{TIME:timestamp}%{SPACE}%{LOGLEVEL:level}%{SPACE}\[%{JAVACLASS:class}\]" ]
    }

    # el @timestamp que inserta logstash es al momento de parsear el evento,
    # el @timestamp lo usa logstash para armar los indices "diarios" y trabajar con rangos para graficar
    # con esto le decimos que use el campo "timestamp" que capturo mas arriba para que pise el @timestamp
    # el timestamp de un jboss tiene la forma "yyyy-MM-dd HH:mm:ss,SSS"
    # mientras que el timestamp de un tomcat para el filter AccessLog es "dd/MMM/yyyy:HH:mm:ss Z"
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS", "dd/MMM/yyyy:HH:mm:ss Z","HH:mm:ss,SSS" ]
      timezone => "America/Argentina/Buenos_Aires"
    }

    #tenemos dos veces el timestamp, host tiene el dato de mi maquina, tmp_path era temporal
    mutate {
      remove_field => [ "timestamp", "tmp_path", "real_host" ]
      remove_tag => [ "_grokparsefailure" ]
    }
}


filter {
  if [type] == "server-log" {

    # del path obtenemos el real_host que es la maquina donde se obtuvieron los logs
    grok {
      match => [ "path", "/%{WORD:patform}/server.log" ]
    }

    mutate {
      replace => [ "patform", "%{platform}" ]
    }

    # cada grok es para un tipo de evento en particular, agregar uno por cada linea que querramos convertir en evento
    #grok {
    #  match => [ "message", "ID: %{NUMBER:idTramite:int}" ]
    #}
    #grok {
    #    match => [ "message", "PAYLOAD: %{GREEDYDATA:payload}" ]
    #  }

    #tenemos dos veces el timestamp, host tiene el dato de mi maquina, tmp_path era temporal
    mutate {
      remove_field => [ "timestamp", "tmp_path", "real_host" ]
      remove_tag => [ "_grokparsefailure" ]
    }
  }
}




########################################################################
##############                  OUTPUT
########################################################################

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
    index => "jboss-%{+YYYY.MM.dd.HH}"
	}
  stdout {
      codec => rubydebug
  }
}
